# Building the Statistical Model {#ANOVAmodel}

The numerical summaries of any study are subject to sampling variability.  That is, if we were to repeat the study with new subjects, the statistics we compute would almost certainly change to some degree.  The key to feeling confident in our results is to quantify the variability in our estimates; this was the argument made in Chapters \@ref(SamplingDistns) and \@ref(NullDistns).  The goal of any statistical analysis is then to develop a model for the sampling (or null) distribution of a statistic.  Often times, this requires modeling the data-generating process as a precursor.  As in any other discipline, statistical models simplify the process being modeled by making certain assumptions.  In this chapter, we develop a model that will help us make inference about the mean of several populations.


## General Formulation
Consider dropping a tennis ball from the top of a 50-meter building and recording the time required before the ball hits the ground.  Applying the principles learned in a first course in physics, we would be able to compute the time precisely using the formula
$$\text{time} = \sqrt{\frac{2(\text{distance})}{9.8}}$$

where $9.8 m/s^2$ is the acceleration due to gravity; further, this formula works regardless of the mass of the object.  Plugging 50 meters into the equation yields a time of `r round(2*50/9.8, 2)` seconds.  If we were to drop a second tennis ball from the same building, the formula tells us that it will also take `r round(2*50/9.8, 2)` seconds to hit the ground below.  This is known as a __deterministic__ system since entering a constant input always results in the same output.

```{definition, label=defn-deterministic-process, name="Deterministic Process"}
One which is completely determined by the inputs.  That is, entering the same input twice will always result in the same output with certainty.
```

This is a model; it simplifies extremely complex processes involving the gravitational pull between objects and works reasonably well.  However, it does not always match reality.  If we were to repeatedly drop tennis balls from the same 50-meter building and record the time before hitting the ground, we might find that the time differs slightly from one ball to the next.  There are several reasons why our observed responses do not line up directly with those predicted by the above equation; for example, our device for measuring time may be subject to some measurement error, a strong gust of wind could alter the results (while the above equation assumes no air resistance), or the person dropping the ball may have inadvertantly increased the initial velocity of the ball.  These reasons, and others, contribute to the observations not lining up with the model.  That is, there is associated noise in the resulting measurements.  A model which incorporates this noise might be written as
$$\text{time} = \sqrt{\frac{2(\text{distance})}{9.8}} + \text{noise}$$

where the noise is not a known quantity.  As a result, this is a __stochastic__ model as the same value for distance may result in different outputs each time.

```{definition, label=defn-stochastic-process, name="Stochastic Process"}
One which has an element of randomness.  That is, the resulting output of the system cannot be predicted with certainty.
```

This leads us to our general formulation for a statistical model:

\begin{equation}
  \text{Response} = f(\text{variables, parameters}) + \text{noise}
  (\#eq:general-model)
\end{equation}

  
The response we observe is the result of two components:

  - A deterministic component which takes the form of a function of variables and unknown parameters.  It is often this component on which we would like to make inference.
  - A stochastic component which captures the unexplained variability in the data generating process.
  
Since the noise is a random element, it has a distribution.  We often make additional assumptions on the structure of this distribution to enable inference on the deterministic component of the model.  We discuss this later in the chapter.

This general model adheres to the idea of partitioning the variability in the response.  It says that a part of the reason the responses differ between subjects is because they have different variables (remember, parameters are fixed for all subjects in a population); part of the reason is unexplained noise.  The overall goal of a statistical model is to give an explanation for why the data is what it is.  How did it come to be?  What process generated the values I have observed?  Our statistical model says that these values have some deterministic component plus some additional noise we cannot explain.  We now turn towards employing this model in the case of comparing the mean response for multiple groups.


## Statistical Model for A Quantitative Response and a Categorical Predictor
For the [Organic Food Case Study](#CaseOrganic), we are comparing the moral expectations (quantitative response) for different food exposures (levels of a categorical variable).  Our model for the data-generating process is best understood in light of the graphic we used to display the data (see Figure \@ref(fig:anovamodel-organic-plot)).

```{r anovamodel-organic-data, echo=FALSE, ref.label="caseorganic-data"}
```

```{r anovamodel-organic-plot, echo=FALSE, fig.cap="Moral expectation scores for students following exposure to various food types."}
min.organic.df <- organic.df %>%
  filter(Food_Condition=="organic") %>%
  summarise(moral_avg = min(moral_avg),
            Food_Condition = Food_Condition[1])

set.seed(123)
ggplot(data = organic.df,
       mapping = aes(y = moral_avg, x = Food_Condition)) +
  geom_jitter(height = 0, width=0.2, alpha = 0.75, colour = "grey75") +
  geom_boxplot(alpha = 0.5, size = 0.75) +
  geom_point(data = min.organic.df, colour = "red") +
  geom_label(data = min.organic.df, 
             mapping = aes(label = round(moral_avg, 2)), 
             colour = "red",
             nudge_y = 0.2, nudge_x = 0.1) +
  stat_summary(fun.y = "mean", geom = "point", size = 4, shape = 17) +
  labs(x = "Food Exposure Group", y = "Moral Expectation Score") +
  theme_bw(12)
```

Let's consider how the value `r round(min.organic.df$moral_avg, 2)`, highlighted red in Figure \@ref(fig:anovamodel-organic-plot), was generated.  As discussed previously, there are two sources of variability in the moral expectation scores (two reasons that the values are not all the same).  One source is the fact that different subjects had different exposures.  That is, one reason the value `r round(min.organic.df$moral_avg, 2)` differs from others observed is because this subject belongs to the organic group and not the comfort or control exposure groups.  As this is something we can explain, it goes into the deterministic portion of the model; it is a function of known variables (group exposure).  Let the function $f(\cdot)$ be such that the input is the group exposure for the $i$-th subject and the output is the mean moral expectation score for that group; this can be represented as a piecewise function:
$$
f\left((\text{Food Exposure Group})_i\right) = \begin{cases}
  \mu_1 & \text{if i-th subject exposed to organic foods} \\
  \mu_2 & \text{if i-th subject exposed to control foods} \\
  \mu_3 & \text{if i-th subject exposed to comfort foods} \end{cases}
$$

Notice that $f(\cdot)$ involves both a variable of interest as well as parameters of interest --- the mean response $\mu_1, \mu_2, \mu_3$ for each of the three groups.  This function is perfectly acceptable, but it is cumbersome to write in a shortened form.  Notice how the function works: it receives an input regarding which group, and it directs you to the appropriate parameter as an output.   We can write this in a compact way as
$$
f\left((\text{Food Exposure Group})_i\right) = \sum_{j=1}^{3} \mu_j \mathbb{I}\left(\text{i-th subject in food exposure group j}\right)
$$

where $\mathbb{I}(\cdot)$ is the indicator function taking value 1 if the event occurs and 0 otherwise.

```{block2, type="rmdkeyidea"}
The deterministic component of a statistical model incorporates the parameters which govern the question of interest.  It is built to explain differences in the response based on differences in group membership or other characteristics of the subjects.
```

This is the deterministic part of the model, as inputing the same group always results in the same output --- the unknown parameter characterizing the mean response for the group.  This, however, only captures one reason we feel the responses differ across subject.  This deterministic component says that every single person exposed to the same food group should have the same moral expectations.  It does not explain why subjects within the organic group do not all share the average moral expectation score.  This source of variability is something we cannot fully explain but attribute to natural variability in this group or measurement error in how we obtained the response.  In order to capture this, we add noise to the system, and we allow this noise to be a random variable which is unique to each subject within the population.  Letting $\epsilon_i$ represent the noise accompanying the response of the $i$-th subject, we can now extend the model in Equation \@ref(eq:general-model) to accommodate these two sourses of variability and obtain
$$
\text{(Moral Expectation Score)}_i = \sum_{j=1}^3 \mu_j \mathbb{I}(\text{i-th subject in food exposure group j}) + \epsilon_i
$$

This may be written in shorthand (suppressing the parameters and noise) as
$$ \text{Moral Expectation Score} \sim \text{Food Exposure Group}$$

```{block2, type="rmdkeyidea"}
The stochastic component of a statistical model captures the unexplained variability due to natural variability in the population or measurement error in the response.
```


```{block2, type="rmdtip"}
In general, given a quantitative response variable $y$, our model for the data generating process comparing this variable across several levels of a factor is
$$y_i = \sum_{j=1}^k \mu_j \mathbb{I}(\text{i-th subject in factor level j}) + \epsilon_i$$

```

In general, students struggle with the fact that we have two different models floating around.  Currently, we are modeling the data-generating process.  This model is used to develop a secondary model of the sampling distribution (or null distribution) of a statistic of interest.  It is this secondary model that is actually necessary in order to conduct inference; the model for the data-generating process is simply a stepping stone to the model of interest.


## Conditions on the Error Distribution
In our model for the data-generating process we incorporated a component $\epsilon$ to capture the noise within each group.  Since the error is a random variable (stochastic element), we know it has a distribution.  We typically assume a certain structure to this distribution.  The more assumptions we are willing to make, the easier the analysis, but the less likely our model is to be applicable to the actual data-generating process we have observed.  The conditions we make dictate how we conduct inference (the computation of a p-value or confidence interval).

The first condition we consider is that the noise attributed to one observed individual is __independent__ of the noise attributed to any other individual observed.  That is, the amount of error in any one individual's response is unrelated to the error in any other response observed.  It is easiest to understand this condition by examining a case when the condition would not hold. 

```{definition, label=defn-independence, name="Independence"}
Two variables are said to be independent when the likelihood that one variable takes on a particular value does not depend on the value of the other variable.  
```

```{example, label=ex-programming, name="Programming Speed"}
Suppose we are conducting a study to compare the speed required to complete a particular programming task in two different languages: Python and R.  We obtain a sample of 100 programmers previously exposed to Java but neither Python nor R.  We ask each programmer to complete a programming exercise in Python and record the time required to successfully complete the task.  Then, we ask each programmer to perform the same task in R and record the time required to successfully complete the task.

The model for the data generating process would be
$$
(\text{Time})_i = \mu_1 \mathbb{I}(\text{i-th task programmed in Python}) + \mu_2 mathbb{I}(\text{i-th task programmed in R}) + \epsilon_i
$$

Given the method in which the data was collected, it would not be reasonable to assume the errors are independent of one another.  Some programmers are naturally faster than others.  A programmer with a below average (negative $\epsilon$) time in Python will most likely have a below average (negative $\epsilon$) time in R on the same task.  Therefore, there is a relationship between the errors for some of the observations taken.  This violates the independence condition.
```

The second condition that is typically placed on the distribution of the errors is that the variability of the responses is similar within each group.  This assumption is known as __homoskedasticity__.

```{definition, label=defn-homoskedasticity, name="Homoskedasticity"}
Also known as "constant variance," this assumption states that the variability of error terms for individuals within a group is the same across all groups.
```

Practically, this means that the responses in one group are not dramatically more variable than any other group (the width of the box portion of a boxplot should be roughly the same across groups).  This condition ensures that the precision of the measurements is roughly similar.  In fact, we made use of this assumption in the construction of our standardized test statistic 
$$T = \frac{MSTrt}{MSE}$$

since MSE was a pooled estimate of the variability.  If we were not willing to assume that the variabilities were similar, we would not construct a pooled estimate.  This also highlights that the MSE is an estimate of the variability of observations within any group when this condition is satisfied.


## Simulating the Null Distribution
We note that this section is a bit more technical than other sections.  We want to give the reader a feel for the computational aspect of simulating the null distribution.  However, understanding conceptually that we are repeating the study in a world in which the null hypothesis is true is sufficient for interpreting a p-value.

Under the above conditions, we can model the null distribution of our standardized test statistic.  The key here is to lean on our data generating process.  Consider the [Organic Food Case Study](#CaseOrganic).  _If the null hypothesis is true_, then we have that
$$\mu_{\text{organic}} = \mu_{\text{comfort}} = \mu_{\text{control}}$$

Let's define this common mean to be $\mu$; we do not know what this value is, but it is common to all groups.  Therefore, _if the null hypothesis is true_, we have that the data generating process reduces to

\begin{equation}
  \text{(Moral Expectation Score)}_i = \mu + \epsilon_i
  (\#eq:null-model)
\end{equation}


Therefore, we can generate data according to this model.  We can replace $\mu$ by our best estimate --- the sample mean response across all observations regardless of their group.  It simply remains to determine how to approximate a random variable from the noise distribution.  In order to do this, we need estimates of the errors, known as __residuals__.

```{definition, label=defn-residual, name="Residual"}
The difference between the observed response and the predicted response (estimated deterministic portion of the model).  Residuals approximate the noise in the data-generating process.
```

The deterministic model gives a way of predicting the response.  For example, consider the [Organic Food Case Study](#CaseOrganic); the data is reproduced in Figure \@ref(anovamodel-organic-boxplot).  Based on the data available, if a subject were to be exposed to organic foods, we would expect their moral expectation score to be `r round(mean(organic.df$moral_avg), 2)`; this is the average observed among individuals randomized to this treatment within our study.

```{r anovamodel-organic-boxplot, echo=FALSE, fig.cap="Comparison of the moral expectations for college students exposed to different types of food."}
ggplot(data = organic.df,
             mapping = aes(y = moral_avg, x = Food_Condition)) +
  geom_jitter(height = 0, width = 0.2, alpha = 0.75, colour = "grey75") +
  geom_boxplot(alpha = 0.5, size = 1.1) +
  labs(x = "Food Exposure Group", y = "Moral Expectation Score") +
  theme_bw(12)
```

That is, we can define the __predicted value__ for $i$-th observation in our study as
$$
\widehat{y}_i = \sum_{j=1}^{3} \bar{y}_j \mathbb{I}(\text{i-th subject in food exposure group j})
$$

and the corresponding residual as
$$
e_i = y_i - \widehat{y}_i
$$

Let's not get lost in the mathematical notation; the residual here is simply the difference between the response of the subject and the average response for their corresponding group.

The key idea here is that residuals approximate the unseen error.  Therefore, if we take this error and perturb it, we can generate new data.  A new dataset, generate under the null hypothesis, can then be constructed as
$$
y_i^* = \bar{y} + e_i^*
$$

where $y_i^*$ is then a new observation constructed by taking a mean and adding a perturbed version of the residual for that observation.  Notice that each newly generated response has the same mean (so that the null is true).  We then take this new dataset and compute the standardized test statistic as before and record it.  Then, we repeat this process over and over again until we have constructed the null distribution.  This gives us a sense of the p-value.


## Recap
We have covered a lot of ground in this chapter, and it is worth taking a moment to summarize the big ideas.  In order to construct a model for the null distribution of the standardized test statistic, we took a step back and modeled the data generating process.  Such a model consists of two components: a deterministic component explaining the differences between groups and a stochastic component capturing the noise in the system.

Certain conditions are placed on the distribution of the noise in our model.  Using these assumptions, we can generate data which adheres to the null hypothesis.  Therefore, we can obtain an empirical model that suggests what values of a test statistic we might expect.
