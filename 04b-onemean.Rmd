# Inference on the Mean of a Single Population (One-Sample t-Tests) {#OneMean}

Throughout the text, we have focused on inference in two contexts:

  - Comparing the mean of a quantitative response across the levels of a factor.
  - Modeling the mean of a quantitative response as a function of one or several predictors.
  
We have seen that our modeling approach is quite flexible and can handle a myriad of research questions.  However, this is not the typical approach for introducing inference to scientists and engineers.  A more common approach is to progress through a series of scenarios, each of which grows in complexity:

  1.  Inference for the mean of a single variable.
  2.  Comparing the mean response for two independent groups.
  3.  Comparing the mean response for several groups (ANOVA).
  4.  Simple linear regression.
  
The third topic in this list was covered in the second unit of this text, and the fourth topic was addressed through consideration of the "marginal relationship" between two variables discussed in the third unit.  It might appear that we have skipped the first two topics; however, the reality is that each of these is simply a special case of ANOVA or Regression.  

In this unit, we address each of these cases in turn and show how they connect to the topics we have already addressed.  The primary motivation for discussing these special cases is to make the reader familiar with terminology which is still widely used within industry.  It also serves as a way of reaffirming the flexibility of the modeling approach we considered throughout the text.


## Framing the Question
Consider the [Birthweight Case Study](#CaseBabies), and suppose our primary question of interest was the following:

  > Is there evidence that the average birthweight of an infant born in North Carolina is below 3500 grams?
  
The parameter of interest is the average birthweight of an infant born in North Carolina.  That is, we are interested in characterizing the mean response for a single population (hence the title of the chapter).  Our hypotheses which capture this question are

  > $H_0:$ the average birthweight of an infant born in North Carolina is at least 3500 grams.  
  > $H_1:$ the average birthweight of an infant born in North Carolina is less than 3500 grams.
  
Letting $\mu$ represent the average birthweight (g) of an infant born in North Carolina, we can write the hypotheses mathematically as

  > $H_0: \mu \geq 3500$  
  > $H_1: \mu < 3500$
  
Figure \@ref(fig:onemean-plot) summarizes the birthweights for the infants in our sample.  The weights tend to hover just below 3500 grams; in fact, the average birthweight for infants in our sample is `r round(mean(babies.df$Weight), 0)` grams.

```{r onemean-plot, echo=FALSE, fig.cap="Birthweights for a sample of infants born in North Carolina during 2004."}
ggplot(data = babies.df,
       mapping = aes(x = Weight)) +
  geom_density(fill = "grey75", colour = "black", size = 1.1) +
  labs(x = "Birthweight (g)", y = "") +
  theme_bw(12) +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
```


## Classical Approach
The classical method of addressing this question of interest is to conduct a "one-sample t-test."  This procedure defines a standardized test statistic as

$$T = \frac{\bar{x} - \mu_0}{s/\sqrt{n}}$$

where $\bar{x}$ and $s$ represent the sample mean and standard deviation of the sample, respectively; and, $n$ represents the sample size.  The parameter $\mu_0$ represents the mean response if the null hypothesis were true.  For our sample, we have that

$$T = \frac{`r round(mean(babies.df$Weight),0)` - 3500}{`r round(sd(babies.df$Weight), 0)`} = `r round((mean(babies.df$Weight)-3500)/(sd(babies.df$Weight)/sqrt(nrow(babies.df))), 2)`$$

A model for the null distribution of this standardized test statistic is derived under two conditions:

  1.  The birthweight for one infant is independent of the birthweight for any other infant.
  2.  The birthweights for infants in the population follows a Normal distribution.
  
Under these two conditions, we have an analytical model for the null distribution of the standarized test statistic.  The probability model which corresponds to the null distribution is called the "t-distribution", hence the name "one-sample t-test."  

Figure \@ref(fig:onemean-t-test) shows the model for the null distribution and the computation of the corresponding p-value.  Based on the p-value, there is strong evidence the birthweight for infants born in North Carolina is less than 3500 grams.  Using a similar analytical model for the sampling distribution, we are able to construct a 95% confidence interval for $\mu$, which is given as (`r round(mean(babies.df$Weight)-qt(0.975, df = nrow(babies.df)-1)*sd(babies.df$Weight)/sqrt(nrow(babies.df)),0)`, ``r round(mean(babies.df$Weight)+qt(0.975, df = nrow(babies.df)-1)*sd(babies.df$Weight)/sqrt(nrow(babies.df)),0)`).

```{r onemean-t-test, echo=FALSE, fig.cap="Null distribution for the standardized test statistic of a one-sample t-test.  The shaded region represents the p-value."}
t.stat <- (mean(babies.df$Weight)-3500)/(sd(babies.df$Weight)/sqrt(nrow(babies.df)))

plot.dat <- data_frame(
  x = seq(-5, 5, length.out = 1000),
  y = dt(x, df = nrow(babies.df) - 1),
  group = x<=t.stat
)

ggplot(data = plot.dat,
       mapping = aes(x = x, y = y, fill = group)) +
  geom_area(color = "black", size = 1.1) +
  labs(x = "Standardized Test Statistic Under Null Hypothesis") +
  scale_fill_manual(values = c("TRUE" = "red", "FALSE" = "grey75")) +
  geom_vline(data = data_frame(xintercept = t.stat),
             mapping = aes(xintercept = xintercept),
             colour = "red", linetype = 2, size = 1.1) +
  annotate("label", x = t.stat, y = 0.1, label = "T == -3.37", 
           colour = "red", parse = TRUE) +
  annotate("label", x = t.stat, y = 0.3, label = "p = 0.0004", 
           colour = "red", parse = FALSE) +
  guides(fill = "none") +
  theme_bw(12) +
  theme(axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        axis.ticks.y = element_blank())
```

## Connection to Modeling
The classical one-sample t-test is equivalent to a regression model which has no predictors.  That is, consider the following model for the data generating process:

$$(\text{Birthweight})_i = \mu + \epsilon_i$$

where the following conditions are placed on the error term:

  1.  The errors in the birthweight have a mean of 0.
  2.  The error in the birthweight for one infant is independent of the error in the birthweight for any other infant.
  3.  The errors in the birthweight follow a Normal Distribution.
  
We note that we do not need the condition of "constant variance" since there are no predictors in our model.  This is the same as a regression model with only an intercept term and no predictors.  Therefore, we can estimate the mean using least squares, which results in the results summarized in Table \@ref(tab:onemean-fit).  Notice that the least squares estimate corresponds to $\bar{x}$, the sample mean.

```{r onemean-fit, echo=FALSE}
fit.babies.intonly <- lm(Weight ~ 1, data = babies.df)

fit.babies.intonly %>%
  tidy() %>%
  cbind(confint_tidy(fit.babies.intonly)) %>%
  select(Term = term,
         Estimate = estimate,
         `Standard Error` = std.error,
         `95% LCL` = conf.low,
         `95% UCL` = conf.high,
         `P-Value` = p.value) %>%
  mutate(Term = recode(Term, "(Intercept)" = "Intercept")) %>%
  knitr::kable(digits = 3,
               caption = "Summary of an intercept only model fit to explain the birthweight of babies in our sample.")
```

You might notice that the p-value here differs from the one we computed above.  That is because the hypotheses are different; above, we tested whether the mean was less than 3500, but in the above output, recall that every test is whether the parameter is 0.  If we were to use the regression output to test the same hypothesis, we would be led to the same conclusion.  Further, notice that the confidence interval is the same as above.  In both cases, we are estimating the same parameter.  And, since our assumptions were the same in each of the two approaches (classical t-test and the model approach), we obtain the same solution.

```{block2, type="rmdkeyidea"}
Presented with the same problem, if two analyses result in different p-values or confidence intervals, it is because the underlying assumptions differ.  If two analyses make the same assumptions, the results will be identical.
```

A one-sample t-test is equivalent to a regression model with an intercept only.  The standardized test statistic differs, but both are quantifying the same thing.
