[
["Basics.html", "Statistical Modeling for Engineers and Scientists 1 The Statistical Process 1.1 Overview of Drawing Inference 1.2 Anatomy of a Dataset 1.3 A Note on Codebooks", " Statistical Modeling for Engineers and Scientists Eric M Reyes Last Updated: 2017-07-28 1 The Statistical Process Is driving while texting as dangerous as driving while intoxicated? Is there a relationship between a student’s college grade point average and their starting salary following graduation? If so, does that relationship differ across academic departments? Regardless of your future career path, you will eventually need to answer a question. The discipline of statistics is about using data to address questions by converting that data into valuable information. Statistics is the discipline of converting data into information. It might be natural at this point to ask “do I really need an entire class about answering questions with data? Isn’t this simple?” Sometimes, it is simple; other times, it can be far from it. Let’s illustrate with the following example from Tintle et al. (2015). Example 1.1 (Organ Donation) Even though organ donations save lives, recruiting organ donors is difficult. Interestingly, surveys show that about 85% of Americans approve of organ donation in principle and many states offer a simple organ donor registration process when people apply for a driver’s license. However, only about 38% of licensed drivers in the United States are registered to be organ donors. Some people prefer not to make an active decision about organ donation because the topic can be unpleasant to think about. But perhaps phrasing the question differently could affect people’s willingness to become a donor. Johnson and Goldstein (2003) recruited 161 participants for a study, published in the journal Science, to address the question of organ donor recruitment. The participants were asked to imagine they had moved to a new state and were applying for a driver’s license. As part of this application, the participants were to decide whether or not to become an organ donor. Participants were presented with one of three different default choices: Some of the participants were forced to make a choice of becoming a donor or not, without being given a default option (the “neutral” group). Other participants were told that the default option was not to be a donor but that they could choose to become a donor if they wished (the “opt-in” group). The remaining participants were told that the default option was to be a donor but that they could choose not to become a donor if they wished (the “opt-out” group). The results of this study were 79% agreeing to become donors in the neutral group, 42% for the opt-in group, and 82.0% for the opt-out group. The results of the study are presented in Figure 1.1. It seems obvious that using the “opt-in” strategy results in fewer people agreeing to organ donation. However, does the “opt-out” strategy, in which people are by default declared organ donors, result in more people agreeing to organ donation compared to the “neutral” strategy? On the one hand, a higher percentage did agree to organ donation under the “opt-out” (82% compared to 79%). However, since this study involved only a subset of Americans, is this enough evidence to claim the “opt-out” strategy is really superior compared to the “neutral” strategy? The discipline of statistics provides a framework for addressing such ambiguity. Figure 1.1: Summary of the responses for the Organ Donation Study described in Example 1.1. 1.1 Overview of Drawing Inference Let’s begin by taking a step back and considering the big picture of how data is turned into information. Every research question we pose, at its heart, is trying to characterize a population, the group of subjects of ultimate interest. Definition 1.1 (Population) The collection of subjects we would like to say something about. In the Organ Donation study, the researchers would like to say something about Americans who are of the age to consent to organ donation; in particular, they would like to quantify how likely it is that someone from this group agrees to organ donation. Therefore, the population is the all Americans who are of the age to consent to organ donation. The subjects in a population need not be people; the population could just as easily be a collection of screws, sheet metal…whatever characterizes the objects from which we would like to obtain measurements. We use the phrase “like to” because in reality, it is often impossible (or impractical) to observe the entire population. Instead, we make observations on a subset of the population; this smaller group is known as the sample. Definition 1.2 (Sample) The collection of subjects for which we actually obtain measurements (data). For each subject within the sample, we obtain a collection of measurements forming our set of data. The goal of statistical modeling is to use the sample (the group we actually observe) to say something about the population of interest (the group we wish we had observed); this process is known as statistical inference. This process is illustrated in Figure 1.2. Definition 1.3 (Statistical Inference) Sometimes referred to as “inference,” the process of using a sample to characterize some aspect of the population. Figure 1.2: Illustration of the statistical process. 1.2 Anatomy of a Dataset Once we have our sample, we take measurements on each of the subjects. These measurements form the data. When we hear the word “data,” most of us envision a large spreadsheet. In reality, data can take on many forms — spreadsheets, images, text files, unstructured text from a Twitter feed, etc. Regardless of the form, all datasets contain information for each subject in the sample; this information, the various measurements, are called variables. Definition 1.4 (Variable) A measurement, or category, describing some aspect of the subject. Variables come in one of two flavors. Categorical variables are those which denote a grouping to which the subject belongs. Examples include marital status, brand, and experimental treatment group. Numeric variables are those which take on values for which ordinary arithmetic (addition, multiplication) makes sense. Examples include height, age of a product, and diameter. Note that sometimes numeric values are used to represent the levels of a categorical variable in a dataset; for example, 0 may indicate “No” and 1 may indicate “Yes” for a variable capturing whether a person is a registered organ donor. Therefore, just because a variable has a numeric value does not make it a numeric variable; the key here is that numeric variables are those for which arithmetic makes sense. Definition 1.5 (Categorical Variable) Also called a “qualitative variable,” a measurement on a subject which denotes a grouping or categorization. Definition 1.6 (Numeric Variable) Also called a “quantitative variable,” a measurement on a subject which takes on a numeric value and for which ordinary arithmetic makes sense. While it may be natural to think of a dataset as a spreadsheet, not all spreadsheets are created equal. Here, we consider datasets which have the following characteristics: Each column contains a unique variable. Each record (row in the dataset) corresponds to a different observation of the variables. If you have multiple datasets, they should include a column in the table that allows them to be linked (subject identifier). These are characteristics of “tidy data.” Even unstructured data such as images or Twitter feeds must be processed, often converted to tidy data, prior to performing a statistical analysis. The above description eliminates a common method of storing data in engineering ans scientific disciplines — storing each sample in a different column. To illustrate, suppose we conduct a study comparing the lifetime (in hours) of two brands of batteries. We measure the lifetime of five batteries of Brand A and six of Brand B. It is common to see a dataset like that in Table 1.1; the problem here is that the first record of the dataset contains information on two different observations. We have the lifetime from a battery of Brand A in the same row as the lifetime from a battery of Brand B. Table 1.1: Example of a common data structure which does not represent tidy data. Data is from a hypothetical study comparing battery lifetimes (hours). Brand A Brand B 8.3 8.4 5.1 8.6 3.3 3.8 5.3 4.1 5.7 4.5 4.0 In order to adhere to the tidy structure, we can reformat this dataset as illustrated in Table 1.2. Here, each record represents a unique observation and each column is a different variable. We have also added a unique identifier. Table 1.2: Example of a tidy dataset, a good way of storing data. Data is from a hypothetical study comparing battery lifetimes (hours). Battery Brand Lifetime 1 A 8.3 2 A 5.1 3 A 3.3 4 A 5.3 5 A 5.7 6 B 8.4 7 B 8.6 8 B 3.8 9 B 4.1 10 B 4.5 11 B 4.0 It may take some time to get used to storing data in this format, but it makes analysis easier and avoids time spent managing the data later. 1.3 A Note on Codebooks A dataset on its own is meaningless if you cannot understand what the values represent. Before you access a dataset, you should always review any available codebooks. Definition 1.7 (Codebook) Also called a “data dictionary,” these provide complete information regarding the variables contained within a dataset. Some codebooks are excellent, with detailed descriptions of how the variables were collected and appropriate units. Other codebooks are not as good, giving only an indication of what the variable represents. Whenever you are working with previously collected data, reviewing a codebook is the first step; and, you should be prepared to revisit the codebook often throughout an analysis. When you are collecting your own dataset, constructing a codebook is essential for others to make use of your data. References "],
["CaseGreece.html", "2 Case Study: Seismic Activity in Greece", " 2 Case Study: Seismic Activity in Greece At the intersection of the African plate, the Eurasia plate, and the smaller Aegean plate, Greece is one of the most earthquake-prone regions in the world. Between July 2016 and July 2017, Greece experienced 179 earthquakes; by contrast, the state of Texas experienced 28 over the same span of time. In a region with such seismic activity, careful consideration must be given to municipal construction. Further, understanding how the motion experienced in a location is related to the soil properties in the area or the magnitude and distance of an earthquake is important. An article in the Journal of Earthquake Engineering (Koutrakis et al. 2002) examined seismic events in Greece occurring between 1978 and 1997. Of interest for construction is characterizing the “strong ground motion,” when the earth shakes with enough force to cause damage to infrastructure, with respect to the properties of a location. The study recorded several measurements for a collection of 121 seismic events1. The primary variable of interest is the uniform duration, the amount of time, in seconds, the ground acceleration (which occurs when the earth is shaking) exceeds a specified value. For this study, the specified threshold was twice the acceleration due to gravity. In addition, the following measurements were available for each observation: magnitude of the nearby earthquake on the Richter scale (larger values indicate more severe earthquakes) distance in kilometers of the measurement from the epicenter of the earthquake indicator (Yes/No) of whether the local soil consists of soft alluvial deposits (alluvial deposits consist fine particles of silt and clay as well as larger particles of gravel and sand). indicator (Yes/No) of whether the local soil consists of pre-Quaternary rocks (rocks older than 2.58 million years) The first 5 observations in the dataset are shown in Table 2.1. While there are several question one might ask with the available data, we first discuss how we might characterize the uniform duration of these events; that is the emphasis of Part I of the text. Table 2.1: Data for first 5 observations from study characterizing seismic activity in Greece. Uniform Duration (s) Magnitude Distance from Epicenter (km) Soft Alluvial Deposits Present Quaternary Rock Present 8.82 6.4 30 Yes No 4.08 5.2 7 No No 15.90 6.9 105 Yes No 6.04 5.8 15 No No 0.15 4.9 16 Yes No References "],
["Questions.html", "3 Asking the Right Questions 3.1 Characterizing a Variable 3.2 Framing the Question", " 3 Asking the Right Questions The discipline of statistics is about turning data into information in order to address some question. While there may be no such thing as a stupid question, there are most certainly ill-posed questions — those which cannot be answered as stated. Consider the Seismic Activity Case Study. It might seem natural to ask “does the uniform duration exceed 5 s for seismic events in Greece?” However, we quickly see that this is an ill-posed question. The first observation in the sample has a uniform duration of 8.82 s, while the second observation has a uniform duration of only 4.08 s. The answer to our question, therefore, seems to be “sometimes.” In fact, looking at a plot of the data (Figure 3.1) shows that the uniform duration for those observations in our sample can be quite different. Figure 3.1: Uniform duration (s) for 121 seismic events in Greece. So, variability makes some questions ill-posed. It is variability that creates a need for statistics; in fact, you could think of statistics as the study and characterization of variability. We must therefore learn to ask the right questions — those which can be answered in the presence of variability. Definition 3.1 (Variability) The notion that measuremenets differ from one observation to another. The presence of variability makes some questions ill-posed; statistics concerns itself with how to address questions in the presence of variability. 3.1 Characterizing a Variable Since we want to say something about the population, any question we ask should then be centered on this larger group. The first step to constructing a well-posed question is then to identify the population of interest for our study. For the Seismic Activity Case Study, it is unlikely that we are only interested in these 121 observed events between 1978 and 1997. In reality, the 121 observations in our dataset form the sample, a subset from all seismic events that occur in Greece. That is, our population of interest is comprised of all seismic events in Greece (both past and future). When identifying the population of interest, be specific! Are you really interested in all trees, for example? Or, are you interested in Maple trees within the city limits of Terre Haute, Indiana? Since we can expect that the uniform duration (the variable of interest for this study) varies from one seismic event to the next, we cannot ask a question about the value of the uniform duration itself (since the value differs for each subject). Instead, we want to characterize the distribution of the uniform duration. Definition 3.2 (Distribution) The pattern of variability corresponding to a set of values. There are many ways to characterize a distribution, many of which are discussed in the chapter Summarizing the Evidence. The most common such characterization is the mean, or average. Such numeric quantities which summarize the distribution of the variable within the population are known as parameters. Definition 3.3 (Parameter) Numeric quantity which summarizes the distribution of a variable within the population of interest. While the value of a variable may vary across the population, the parameter is a single fixed constant which summarizes the variable for the population. Therefore, well-posed questions can be constructed if we limit ourselves to questions about the parameter. The second step in constructing well-posed questions is then to identify the parameter of interest. The questions we ask then generally fall into one of two categories: Estimation: what is the average uniform duration for seismic events in Greece? Model Consistency: is it reasonable the average uniform duration is similar for sesimic events regardless of whether the soil consists of alluvial deposits, or is there evidence that the average uniform duration differs between these two types of soil? Now, since we do not get to observe the population (we only see the sample), we cannot observe the value of the parameter. That is, we will never know the average uniform duration for all seismic events in all locations in Greece. However, we can use the sample to inform us of what the data suggests. Definition 3.4 (Estimation) Using the sample to approximate the value of a parameter from the underlying population. Definition 3.5 (Hypothesis Testing) Using a sample to determine if the data is consistent with a working theory or if there is evidence to suggest the data is not consistent with the theory. Parameters are unknown values and can, in general, never be known. Parameters are generally denoted by Greek letters in statistical formulas. It turns out, the vast majority of research questions can be framed in terms of a parameter. In fact, this is the first of what we consider the Five Fundamental Ideas of Inference. Fundamental Idea I: A research question can often be framed in terms of a parameter which characterizes the population. Framing the question should then guide our analysis. 3.2 Framing the Question In engineering and scientific applications, many questions fall under the second category of model consistency. Examining such questions is known as hypothesis testing. Specifically, data is collected to help the researcher choose between two competing theories for the parameter of interest. In this section, we consider the terminology surrounding specifying such questions. For the Seismic Activity Case Study suppose we are interested in addressing the following question: Is there evidence that the average uniform duration for seismic events in Greece exceeds 5 seconds? The question itself is about the population (seismic events in Greece) and is centered on a parameter (the average uniform duration). That is, this is a well-posed question that can be answered with appropriate data. The overall process for addressing these types of questions is similar to conducting a trial in a court of law. In the United States, a trial has the following essential steps: Assume the defendant is innocent. Evidence to establish guilt (to the contrary of innocence) is presented by the prosecution. The jury considers the weight of the evidence. If the evidence is “beyond a reasonable doubt,” the jury declares the defendant guilty; otherwise, the jury declares the subject not guilty. The process of conducting a hypothesis test has similar essential steps: Assume the innocent of what we want the data to show (develop a working theory). Gather data and compare it to the proposed model. Quantify the likelihood of our data under the proposed model. If the likelihood is small, conclude the data is not consistent with the working model (there is evidence for what we want to show); otherwise, conclude the data is consistent with the working model (there is no evidence for what we want to show). Notice that a trial centers not on proving guilt but on disproving innocence; similarly, in statistics, we are able to establish evidence against a specified theory. This overall process has several other subtle points, and it is okay if the process is not yet clear. Future chapters will discuss methods for actually implementing this process and discuss each step in greater detail. Here, we focus solely on that first step — developing a working theory that we want to disprove. Consider the above question for the Seismic Activity Case Study. We want to show that the average uniform duration exceeds 5 s. Therefore, we would like to disprove (or provide evidence against) the statement that the average uniform duration for seismic events in Greece is no more than 5 s. This is known as the null hypothesis; the opposite of this statement, called the alternative hypothesis, captures what we would like to establish. Definition 3.6 (Null Hypothesis) The statement (or theory) that we would like to disprove. This is denoted \\(H_0\\), read “H-naught” or “H-zero”. Definition 3.7 (Alternative Hypothesis) The statement (or theory) capturing what we would like to provide evidence for; this is the opposite of the null hypothesis. This is denoted \\(H_1\\) or \\(H_a\\), read “H-one” and “H-A” respectively. For the Seismic Activity Case Study, we write: \\(H_0:\\) The average uniform duration of seismic events in Greece is no more than 5 s. \\(H_1:\\) The average uniform duration of seismic events in Greece exceeds 5 s. Each hypothesis is a well-posed statement (about a parameter characterizing the entire population). The two statements are exactly opposite of one another; only one can be a true statement. We can now collect data to determine if it is consistent with the null hypothesis (a statement similar to “not guilty”) or if the data provides evidence against the null hypothesis and in favor of the alternative (a statement similar to “guilty”). Often these statements are written in a bit more of a mathematical structure in which a Greek letter is used to represent the parameter of interest. For example, we might write Let \\(\\theta\\) be the average uniform duration, in seconds, for seismic events in Greece. \\(H_0: \\theta \\leq 5\\) \\(H_1: \\theta &gt; 5\\) In the above statements, \\(\\theta\\) represents the parameter of interest; the value 5 is known as the null value. Definition 3.8 (Null Value) The value associated with the equality component of the null hypothesis; it forms the threshold or boundary between the two hypothesis. Note: not all questions of interest require a null value be specified. Hypothesis testing is a form of statistical inference in which we quantify the evidence against a working theory (captured by the null hypothesis). We essentially argue that the data supports the alternative if it is not consistent with the working theory. Process for Framing a Question In order to frame a research question, consider the following steps: Identify the population of interest. Identify the parameter(s) of interest. Determine if you are interested in estimating the parameter(s) or quantifying the evidence against some working theory. If you are interested in testing a working theory, make the null hypothesis the working theory and the alternative the exact opposite statement (what you want to provide evidence for). "],
["Data.html", "4 Gathering the Evidence (Data Collection) 4.1 What Makes a Sample Reliable 4.2 Poor Methods of Data Collection 4.3 Preferred Methods of Sampling 4.4 Two Types of Studies", " 4 Gathering the Evidence (Data Collection) Consider again the goal of statistical inference — to use a sample as a snapshot to say something about the underlying population (Figure 4.1). This generally provokes unease in people, leading to a distrust of statistical results. In this section we take on that distrust head on. Figure 4.1: Illustration of the statistical process (reprinted from Chapter 1). 4.1 What Makes a Sample Reliable If we are going to have some amount of faith in the statistical results we produce, we must have data we find to be reliable. The Treachery of Images is a canvas painting depicting a pipe, below which the artist wrote the French phrase for “This is not a pipe” (Figure 4.2). Regarding the painting, the artist said The famous pipe. How people reproached me for it! And yet, could you stuff my pipe? No, it’s just a representation, is it not? So if I had written on my picture “This is a pipe,” I’d have been lying! Figure 4.2: The Treachery of Images by René Magritte. Just as a painting is a representation of the object it depicts, so a sample should be a representation of the underlying population from which it was taken. This is the primary requirement if we are to rely on the resulting data. In order for a statistical analysis to be reliable, the sample must be representative of the underlying population. We need to be careful to not get carried away in our expectations. What constitutes “representative” really depends on the question, just as an artist chooses his depiction based on how he wants to represent the object. Let’s consider the following example. Example 4.1 (School Debt) In addition to a degree, college graduates also tend to leave with a large amount of debt due to college loans. In 2012, a graduate with a student loan had an average debt of $29,400; for graduates from private non-profit institutions, the average debt was $32,3002. Suppose we are interested in determining the average amount of debt in student loans carried by a graduating senior from Rose-Hulman Institute of Technology, a small private non-profit engineering school. There are many faculty at Rose-Hulman who choose to send their children to the institute. Since I am also on the faculty, I know many of these individuals. Suppose I were to ask each to report the amount of student loans their children carried upon graduation from Rose-Hulman. I compile the 25 responses and compute the average amount of debt. Further, I report that based on this study, there is significant evidence that the average debt carried by a graduate of Rose-Hulman is far below the $32,300 reported above (great news for this year’s graduating class)! Why might we be hesitant to trust these results? When our distrust of a statistical result stems from a distrust of the data on which it is based, it is generally a result of our doubting the sample is representative of the population. Rose-Hulman, like many other universities, has a policy that the children of faculty may attend their university (assuming admittance) tuition-free. We would therefore expect their children to carry much less debt than the typical graduating senior. This provides a nice backdrop for discussing what it means to be representative. First, let’s define our population; in this case, we are interested in graduating seniors. The variable of interest is the amount of debt carried in student loans; the parameter of interest is then the average amount of debt in student loans carried by graduating seniors. With regard to the grade point average of the students in our sample, it is probably similar to all graduating seniors. Their starting salary is probably similar; the fraction of mechanical engineering majors versus math majors is probably similar. So, in many regards the sample is representative of the population; however, it fails to be representative with regard to the variable of interest. This is our concern. The amount of debt carried by students in our sample is not representative of that debt carried by all graduating seniors. When thinking about whether a sample is representative, focus your attention to the characteristics specific to your research question. Does that mean the sample is useless? Yes and no. The sample collected cannot be used to answer our initial question of interest. No statistical method can fix bad data; that is, statistics adheres to the “garbage-in, garbage-out” phenomena. If the data is bad, no analysis will undo that. While the sample cannot be used to answer our initial question, it could be used to address a different question: What is the average amount of debt in student loans carried by graduating seniors from Rose-Hulman whose parent is a faculty member at the university? For this revised question, the sample is indeed representative. If we are working with previously collected data, we must consider the population to which our results will generalize. That is, for what population is the given sample representative. If we are collecting our data, we need to be sure we collect data in such a way that the data is representative. Let’s first look at what not to do. 4.2 Poor Methods of Data Collection Example 4.1 is an example of a “convenience sample,” when the subjects in the sample are chosen simply due to ease of collection. Examples include surveying students only in your sorority when you are interested in all females who are part of a sorority on campus; taking soil samples from only your city when you are interested in the soil for the entire state; and, obtaining measurements from only one brand of phone, because it was the only one you could afford on your budget, when you are interested in studying all cell phones on the market. A convenience sample is unlikely to be representative if there is a relationship between the ease of collection and the variable under study. This was true in the School Debt example; the relationship of a student to a faculty member was directly related to the amount of debt they carried. As a result, the resulting sample was not representative of the population. When conducting a survey with human subjects, it is common to only illicit responses from volunteers. Such “volunteer samples” tend to draw in those with extreme opinions. Consider product ratings on Amazon. Individual ratings tend to cluster around 5’s and 1’s. This is because those customers who take time to submit a review (which is voluntary) tend to be those who are really thrilled with their product (and want to encourage others to purchase it) and those who are really disappointed with their purchase (and want to encourage others to avoid it). Such surveys often fail to capture those individuals in the population who have intermediate opinions. We could not possibly name all the poor methods for collecting a sample; but, these methods all share something in common — it is much more likely the resulting sample is not representative. Failing to be representative results in biased estimates of the parameter. Definition 4.1 (Bias) A set of measurements, or an estimate of a parameter, is said to be biased if they are consistently too high (or too low). To illustrate the concept of bias, consider shooting at a target as in Figure 4.3. We can consider the center of our target to be the parameter we would like to estimate within the population. The values in our sample (the strikes on the target) will vary around the parameter; while we do not expect any one value to hit the target precisely, a “representative” sample is one in which the values tend to be clustered about the parameter (unbiased). When the sample is not representative, the values in the sample tend to cluster off the mark (biased). Notice that to be unbiased, it may be that not a single value in the sample is perfect, but aggregated together, they point in the right direction. So, bias is not about an individual measurement being an “outlier,” (more on those in a later chapter) but about repeatedly shooting in the wrong direction. Figure 4.3: Illustration of bias and variability. Biased results are typically due to poor sampling methods that result in a sample which is not representative of the underlying population. The catch (there is always a catch) is that we will never know if a sample is representative or not. But, we can employ methods that help to minimize the chance that the sample is biased. 4.3 Preferred Methods of Sampling No method guarantees a perfectly representative sample; but, we can take measures to reduce or eliminate bias. A useful strategy is to employ randomization. This is summarized in our second Fundamental Idea: Fundamental Idea II: If data is to be useful for making conclusions about the population, a process referred to as drawing inference, proper data collection is crucial. Randomization can play an important role ensuring a sample is representative and that inferential conclusions are appropriate. Consider the School Debt example again. Suppose instead of the strategy described there, we had constructed a list of all graduating seniors from the university. We placed the name of each student on an index card; then, I thoroughly shuffle the cards and choose the top 25 cards. For these 25 individuals, I record the amount of debt in student loans each carries. Using a lottery to select the sample is known as a simple random sample. By conducting a lottery, we make it very unlikely that our sample consists of only students with a very small amount of student debt (as occurred when we used a convenience sample). Definition 4.2 (Simple Random Sample) Often abbreviated SRS, this is a sample of size \\(n\\) such that every collection of size \\(n\\) is equally likely to be the resulting sample. This is equivalent to a lottery. There are situations in which a simple random sample does not suffice. Again, consider our School Debt example. The Rose-Hulman student body is predominantly domestic, with only about 3% of the student body being international students. But, suppose we are interested in comparing the average debt carried between international and domestic students. It is very likely that in a simple random sample of 25 students, none will be international by chance alone. Instead of a simple random sample, we might consider taking a sample of, say 13, domestic students and a sample of 12 international students; this is an example of a stratified random sample. This approach is useful when there is a natural grouping of interest within the population. Definition 4.3 (Stratified Random Sample) A sample in which the population is first divided into groups, or strata, based on a characteristic of interest; a simple random sample is then taken within each group. There are countless sampling techniques used in practice. The two described above can be very useful starting point for developing a custom method suitable for a particular application. Their benefit stems from their use of randomization. This section is entitled “Preferred Methods” because while these methods are ideal, they are not always practical. For example, consider the Seismic Activity Case Study; it would be impossible to make a list of all seismic activity that has ever or will ever occur in Greece and then take a simple random sample from that list. Again, our key is to obtain a representative sample; while randomization may be a nice tool for accomplishing this, we may appeal to the composition of the sample itself to justify its use. 4.4 Two Types of Studies Thinking about how the data was collected helps us determine how the results generalize beyond the sample itself (to what population the results apply). When our question of interest is about the relationship between two variables (as most questions are), we must also carefully consider the study design. Too often separated from the statistical analysis that follows, keeping the study design in mind should guide the analysis as well as inform us about the conclusions we can draw. In order to illustrate how study design can impact the results, consider the following example. Example 4.2 (Kangaroo Care) At birth, infants have low levels of Vitamin K, a vitamin needed in order to form blood clots. Though rare, without the ability for her blood to clot, an infant could develop a serious bleed. In order to prevent this, the American Academy of Pediatrics recommends that all infants be given a Vitamin K shot shortly after birth in order to raise Vitamin K levels. As with any shot, there is typically discomfort to the infant, which can be very discomforting to new parents. Kangaroo Care is a method of holding a baby which emphasizes skin-to-skin contact. The child, who is dressed only in a diaper, is placed upright on the parent’s bare chest; a light blanket is draped over the child. The method was initially recognized for its benefits in caring for pre-term infants. Suppose suppose we are interested in determining if utilizing the method while giving the child a Vitamin K shot reduces the discomfort in the infant, as measured by the total amount of time the child cries following the shot. Contrast the following two potential study designs: We allow the attending nurse to determine whether Kangaroo Care is initiated prior to giving the Vitamin K shot. Following the shot, we record the total time (in seconds) the child cries. We flip a coin. If it comes up heads, the nurse should have the parents implement Kangaroo Care prior to giving the Vitamin K shot; if it comes up tails, the nurse should give the Vitamin K shot without implementing Kagaroo Care. Following the shot, we record the total time (in seconds) the child cries. Note, in both study designs (A) and (B), we only consider term births which have no complications to avoid potential complications that might alter the timing of the Vitamin K shot or the ability to implement Kangaroo Care. Note that there are some similarities in the two study designs: The underlying population is the same for both designs — infants born at term with no complications. There are two treatment groups in both designs — the “Kangaroo Care” group and the “no Kangaroo Care” group. The response (variable of interest) is the same in both designs — the time (in seconds) the infant cries. There is action taken by the researcher in both designs — a Vitamin K shot is given to the child. There is one prominent difference between the two study designs: For design (A), the choice of Kangaroo Care is left up to the nurse (self-selected); for design (B), the choice of Kangaroo is assigned to the nurse by the researcher, and this selection is made at random. Design (A) is an example of an observational study; design (B) is a controlled experiment. Definition 4.4 (Observational Study) A study in which the subjects self-select into the treatment groups under study. Definition 4.5 (Controlled Experiment) A study in which the subjects are randomly assigned to the treatment groups under study. It is common to think that anytime the environment is “controlled” by the researcher that an experiment is taking place, but the defining characteristic is the random assignment to treatment groups (sometimes referred to as the factor under study). In the example above, both study designs involved a controlled setting (the delivery room of a hospital) in which trained staff (the nurse) delivered the shot. However, only design (B) is a controlled experiment because the researchers randomly determined into which group the infant would be placed. To understand the impact of randomization, suppose that we had conducted a study as in design (A); further, the results suggest that those infants who were given a shot while using Kangaroo Care cried for a shorter time period, on average. Can we conclude that it was the Kangaroo Care that led to the shorter crying time? Maybe. Consider the following two potential explanations for the resulting data: Kangaroo Care is very effective; as a result, those children who were given Kangaroo Care had reduced crying time following the Vitamin K shot. It turns out that those nurses who chose to implement Kangaroo Care (remember, they have a choice under design (A) whether they implement the method) were also the nurses with a gentler bedside manner. Therefore, these nurses tended to be very gentle when giving the Vitamin K shot whereas the nurses who chose not to implement Kangaroo Care tended to just jab the needle in when giving the shot. As a result, the reduced crying time is not a result of the Kangaroo Care but the manner in which the shot was given. The problem is that we are unable to determine which of the explanations is true. Given the data we have collected, we are unable to tease out the effect of the Kangaroo Care from that of the nurse’s bedside manner. As a result, we are able to say we observed a relationship between the use of Kangaroo Care and reduced crying time, but we are unable to conclude that Kangaroo Care caused a reduction in the crying time. In this hypothetical scenario, the nurse’s bedside manner is called a confounder. Definition 4.6 (Confounding) When the effect of a variable on the response is mis-represented due to the presence of a third, potentially unobserved, variable known as a confounder. Confounders can mask the relationship between the factor under study and the response. There is a documented relationship between ice cream sales and the risk of shark attacks. As ice cream sales increase, the risk of a shark attack also increases. This does not mean that if a small city in the Midwest increases its ice cream sales that the citizens are at higher risk of being attacked by a shark. As Figure 4.4 illustrates, there is a confounder — temperature. As the temperatures increase, people tend to buy more ice cream; as the temperature increases, people tend to go to the beach increasing the risk of a shark attack. Two variables can appear to be related as a result of a confounder. Figure 4.4: Illustration of a confounding variable. The confounder, related to both the factor and the treatment can make it appear as though there is a causal relationship when none exists. Confounders are variables that influence both the factor of interest and the response. Observational studies are subject to confounding; thus, controlled experiments are often considered the gold standard in research because they allow us to infer cause-and-effect relationships from the data. Why does the randomization make such an impact? Because the randomization removes the impact of confounders. Let’s return to the hypothetical study. Suppose there are nurses with a gentle bedside manner and those who are a little less gentle. If the infants are randomized to Kangaroo Care, then for every gentle nurse who is told to implement Kangaroo Care while giving the shot, there is a gentle nurse who is told to not implement Kangaroo Care. Similarly, for every mean nurse who is told to implement Kangaroo Care while giving a shot, there is a mean nurse who is told to not implement Kangaroo Care. This is illustrated in Figure 4.5. For an observational study, the treatment groups are unbalanced; there is a higher fraction (11/12 compared to 1/4) of friendly nurses in the Kangaroo Care group compared to the No Kangaroo Care group. For the controlled experiment however, the treatment groups are balanced; there is approximately the same fraction of friendly nurses in both groups. Randomization is the great equalize; it ensures that the two groups are similar in all respects; therefore, any differences we observe between the two groups must be due to the grouping and not an underlying confounding variable. Figure 4.5: Illustration of the impact of randomization in study design. For the observational study, the treatment groups are unbalanced. For the controlled experiment, the treatment groups are balanced. Randomly assigning treatment groups balances the groups with respect to the confounders; that is, the treatment groups are similar. Therefore, any differences between the two groups can be attributed to the grouping factor itself. While controlled experiments are a fantastic study design, we should not discount the use of observational studies. Consider the Seismic Activity Case Study; suppose we are interested in the following question: Is there evidence that the average uniform duration differs between locations for which the soil consists of alluvial deposits and locations for which it does not? The response is the uniform duration; the factor defining the treatment groups is whether the soil consists of alluvial deposits. There is no way to conduct a controlled experiment to address this question. We cannot randomize the soil content to various locations. This does not mean the data is useless. But, it does mean we will need to be sure we address the potential confounding in the analysis. We will discuss such methods in the latter half of the text. The big idea is that in order to make causal conclusions, we must be able to state that the two treatment groups are balanced with respect to any potential confounders; randomization is one technique for accomplishing this. http://ticas.org/sites/default/files/pub_files/Debt_Facts_and_Sources.pdf↩ "],
["Summaries.html", "5 Presenting the Evidence (Summarizing Data) 5.1 Characteristics of a Distribution", " 5 Presenting the Evidence (Summarizing Data) If you open any search engine and look up “data visualization,” you will be quickly overwhelmed by a host of pages, texts, and software filled with tools for summarizing your data. Here is the bottom line: once you collect data, you must summarize and present it in a way that allows you to address the question of interest. It is both that simple and that complicated. Fundamental Idea III: The use of data for decision making requires that the data be summarized and presented in ways that address the question of interest. This is the goal of any data visualization, no matter how simple or complex — to help turn the data into information to address some question. Pretty pictures for the sake of pretty pictures is not helpful. In this section, we will consider various simple graphical and numerical summaries to help build a case for addressing the question of interest. 5.1 Characteristics of a Distribution "],
["CaseDeepwater.html", "6 Case Study: Health Effects of the Deepwater Horizon Oil Spill", " 6 Case Study: Health Effects of the Deepwater Horizon Oil Spill On the evening of April 20, 2010, the Deepwater Horizon, an oil drilling platform positioned off the coast of Louisiana, was engulfed in flames as the result of an explosion. The drilling rig, leased and operated by BP, had been tasked with drilling an oil well in water nearly 5000 feet deep. Eleven personnel were killed in the explosion. The following clip is from the initial coverage by the New York Times3: Figure 6.1: New York Times coverage of the Deepwater Horizon oil spill. The incident is considered the worst oil spill in US history, creating an environmental disaster along the Gulf Coast. In addition to studying the effects on the local environment, researchers have undertaken studies to examine the short and long-term health effects caused by the incident. As an example, it is reasonable to ask whether volunteers who were directly exposed to oil, such as when cleaning wildlife, are at higher risk of respiratory irritation compared to those volunteers who were helping with administrative tasks and therefore were not directly exposed to the oil. An article appearing in The New England Journal of Medicine (B. D. Goldstein, Osofsky, and Lichtveld 2011) reported the results from a health symptom survey performed in the Spring and Summer of 2010 by the National Institute for Occupational Safety and Health. Of 54 volunteers assigned to wildlife cleaning and rehabilitation, 15 reported experiencing “nose irritation, sinus problems, or sore throat.” Of 103 volunteers who had no exposure to oil, dispersants, cleaners, or other chemicals, 16 reported experiencing “nose irritation, sinus problems, or sore throat.” While clearly a larger fraction of volunteers cleaning wildlife in the study reported respiratory symptoms compared to those who were not directly exposed to irritants, would we expect similar results if we were able to interview all volunteers? What about during a future oil spill? Is there evidence that more than 1 in 5 volunteers who clean wildlife will develop respiratory symptoms? What is a reasonable value for the increased risk of respiratory symptoms for those volunteers with direct exposure compared to those without? In Part II of this text, we see the Five Fundamental Ideas of Inference in action. We will consider the manner in which the data was collected, the scope of the question, how the question should be framed, summarizing and presenting the data clearly in a way which aids in addressing the question, and how to quantify the variability in our estimates. This section of the text really serves as the foundation for the statistical thinking and reasoning needed to address more complext questions encountered later in the text. References "],
["CasePaper.html", "7 Case Study: Paper Strength", " 7 Case Study: Paper Strength While electronic records have become the predominant means of storing information, we do not live in a paperless society. Paper products are still used in a variety of applications ranging from printing reports and photography to packaging and bathroom tissue. In manufacturing paper for a particular application, the strength of the resulting paper product is a key characteristic. There are several metrics for the strength of paper. A conventional metric for assessing the inherent (not dependent upon the physical characteristics, such as the weight of the paper, which might have an effect) strength of paper is the breaking length. This is the length of a paper strip, if suspended vertically from one end, that would break under its own weight. Typically reported in kilometers, the breaking length is computed from other common measurements. For more information on paper strength measurements and standards, see the following website: http://www.paperonweb.com A study was conducted at the University of Toronto to investigate the relationship between pulp fiber properties and the resulting paper properties (Lee 1992). The breaking length was obtained for each of the 62 paper specimens, the first 10 measurements of which are shown in Table 7.1. The complete data is available online at the following website: https://vincentarelbundock.github.io/Rdatasets/doc/robustbase/pulpfiber.html While there are several question one might ask with the available data, we must first begin discuss how we might characterize the breaking length of these paper specimens. We consider various techniques for summarizing a variable. Table 7.1: Breaking length (km) for first 10 specimens in the Paper Strength study. Specimen Breaking Length 1 21.312 2 21.206 3 20.709 4 19.542 5 20.449 6 20.841 7 19.060 8 18.597 9 19.346 10 18.720 References "],
["references.html", "References", " References "]
]
