# Assessing the Evidence (Quantifying the Variability in Estimates) {#SamplingDistn}

Let's again return to the goal of statistical inference --- to use a sample as a snapshot to say something about the underlying population (Figure \@ref(fig:samplingdistn-statistical-process)).  There are generally three reasons people distrust this process:

  1. Fear that the sample does not represent what is going on in the population.
  2. Fear that we cannot make a conclusion with a sample of size $n$ (wanting more data).
  3. Fear that one study is not enough to make a conclusion.

```{r samplingdistn-statistical-process, echo=FALSE, ref.label="basics-statistical-process", fig.cap="Illustration of the statistical process (reprinted from Chapter 1)."}
```

We have already tackled the first reason in Chapter \@ref(#Data); if we are to trust statistical results, we must collect data that is representative of the underlying population.  The second and third fears stated above are tied together, though maybe not obviously.  Our gut reaction to an article with the title "New Study Finds..." is to say "yeah, but I bet if they did the study again, they would get a different answer."  How do we know whether a study actually provides _evidence_?  The underlying mechanisms are discussed in this chapter and how we use these mechanisms is the subject of the next chapter.


## The Ideal Scenario
We begin by entering into this fear of "one study is not enough."  In order to do this, consider the following hypothetical study:

```{example, label=samplingdistn-currency, name="Currency in Circulation"}
Each year, the United States Treasury introduces new currency into the economy.  Here, we are not talking about the economics of this process but limit our discussion to the physical currency printed.  There are many reasons to introduce new currency, including updated security to prevent conterfeiting and to replace worn out currency.  However, when new currency is introduced, older currency is not immediately removed from the economny.  For example, at any point in time there are both new \$20 bills and older \$20 bills available.  Suppose we am interested in estimating the average age of a \$20 bill in current circulation.
```

Based on our discussion of the statistical process so far, the first step here is to take a sample of \$20 bills in circulation which is representative of all \$20 bills in circulation as it is impractical to obtain the age of every \$20 bill in circulation.  Within the sample of $n$ bills obtained, we could compute the average age; since the sample is representative, this value should be a good estimate of the average age of \$20 bills within the population.  That is the process illustrated above in Figure \@ref(fig:samplingdistn-statistical-process).  Now, we might say "but this is just one study of $n$ bills; if we were to get a different sample, things would change."  Absolutely; if we repeat the study by getting a _new_ sample, we will be observing _new_ subjects, recording _new_ values for the variable of interest and therefore obtaining a _new_ estimate.  In fact, this logic then suggests not to just repeat the study once, but twice; not just twice, but three times, etc.  This repeatedly conducting the same study is illustrated in Figure \@ref(fig:samplingdistn-sampling-distribution).

```{r samplingdistn-sampling-distribution, echo=FALSE, fig.cap="Illustration of repeatedly sampling from a population."}
knitr::include_graphics("./images/SamplingDistn-SamplingDistn.jpg")
```

Consider what we have done in this thought experiment.  With each representative sample, we have constructed an estimate of the parameter.  That is, in the first sample, we create $\bar{x}_1$ (the average age of \$20 bills in circulation based on sample 1) which estimates the parameter $\mu$ (the average age of all \$20 bills in circulation).  In the second sample, we create $\bar{x}_2$ which also estimates the parameter $\mu$; and, we can continue this way for as many samples as we like.  Each sample generates a different estimate of the population parameter since each sample is made of a different collection of subjects from the population.  

So, which one do we trust?  All of them.  Since each sample is representative of the population, each estimate is a good (not perfect) estimate of the parameter.  Since we have all these estimates, we could think about pooling the information from all of them; this is known as the __sampling distribution__.

```{definition, label=defn-sampling-distribution, name="Sampling Distribution"}
The distribution of a _statistic_ across repeated samples.
```

Notice that the sampling distribution is not describing a variable, it is describing a _statistic_.  In order to construct a sampling distribution, we would go through the following steps:

  1. Take a sample; record variables of interest.
  2. Compute the statistic which estimates the parameter.
  3. Repeat steps 1 and 2 a large number of times.
  4. Examine the statistics collected.

So, the sampling distribution is not a plot of the raw values of a variable on individual subjects but a value which summarizes an entire sample.  That is, the unit of observation has changed.  While a sample consists of individual subjects from the population, the sampling distribution consists of individual samples from the population.

Let's return to the [Seismic Activity Case Study](#CaseGreece).  Consider again the following question:

  > Is there evidence that the average uniform duration for seismic events in Greece exceeds 5 seconds?
  
```{r samplingdistn-greece-data, echo=FALSE, ref.label="casegreece-data"}
```
  
For the sample of `r nrow(greece.df)` locations current, we observed that the uniform duration is `r round(mean(greece.df$Uniform_Duration), 2)` seconds.  But, suppose we were to repeatedly collect data for a new set of `r nrow(greece.df)` locations and each time recorded the average for those new locations.  Figure \@ref(fig:samplingdistn-greece-sampling-distribution) shows the distribution of these estimates (sample mean uniform duration). Again, this is not the distribution of the variable itself (that was illustrated in the previous chapter); this is the distribution of sample mean uniform durations we obtain from repeatedly doing the same study with new locations each time.

(ref:figcap-samplingdistn-greece-sampling-distribution) Sampling distribution of the mean uniform duration for seismic events in Greece based on a sample of `r nrow(greece.df)` locations.

```{r samplingdistn-greece-sampling-distribution, echo=FALSE, fig.cap=(ref:figcap-samplingdistn-greece-sampling-distribution), cache=TRUE}
# Obtain Bootstrap Sampling Distribution
set.seed(201708)

boot.df <- greece.df %>%
  bootstrap(m = 10000) %>%
  do(summarise(., Sample_Mean = mean(Uniform_Duration)))


# Create Plot
ggplot(data = boot.df,
       mapping = aes(x = Sample_Mean)) +
  geom_density(size = 1.25, fill = "grey50") +
  labs(x = "Sample Means of Uniform Density (s)", y = "Density") +
  theme_bw(12)
```


More is needed here to really explain this better...


Now, to illustrate the power of sampling distributions.  Remember that each value in the sampling distribution is a good estimate of the underlying parameter.  Therefore, we see that these estimates tend to be between 4 and 6.75 seconds.  This tells us that 10 seconds, for example, is not a reasonable value of the parameter.

```{block2, type="rmdkeyidea"}
The sampling distribution of a statistic is the holy grail for statistical inference.  All of the work in statistics is toward modeling this.
```

