# Analyzing Correlated Responses {#Blockmodel}

Our question of interest in this unit is the same as that in our previous unit:

$$H_0: \mu_1 = \mu_2 = \dotsb = \mu_k \qquad \text{vs.} \qquad H_1: \text{At least one } \mu_j \text{ differs}$$

It seems reasonable to begin with the model for describing a quantitative response as a function of a categorical predictor described in Chapter \@ref(ANOVAmodel).  In this chapter, we extend this model to account for the correlation between responses.


## Statistical Model for Correlated Responses
For the [Frozen Yogurt Case Study](#CaseYogurt), we are comparing the average taste rating for different vendors.  In the previous unit, we would have constructed the following model for the data generating process:

$$(\text{Taste Rating})_i = \mu_1 (\text{East Side})_i + \mu_2 (\text{South Side})_i + \mu_3 (\text{Name Brand})_i + \epsilon_i$$

where

$$
\begin{aligned}
  (\text{East Side})_i &= \begin{cases}
    1 & \text{if i-th rating associated with east side yogurt vendor} \\
    0 & \text{if i-th rating not associated with east side yogurt vendor}
    \end{cases} \\
  (\text{South Side})_i &= \begin{cases}
    1 & \text{if i-th rating associated with south side yogurt vendor} \\
    0 & \text{if i-th rating not associated with south side yogurt vendor}
    \end{cases} \\
  (\text{Name Brand})_i &= \begin{cases}
    1 & \text{if i-th rating associated with name brand yogurt vendor} \\
    0 & \text{if i-th rating not associated with name brand yogurt vendor}
    \end{cases}
\end{aligned}
$$

are indicator variables to capture the various factor levels.  In order to use this model, the first condition we imposed on the error term was that the error in the rating for one observation is independent of the error in the rating for all other observations.  However, for the [Frozen Yogurt Case Study](#CaseYogurt), we know this condition is violated.  If the errors were independent of one another, it would imply the responses are independent of one another.  But, since each subject rated each of the three vendors, the ratings from the same subject are related.

It should not come as a surprise that the way to address this complication is to _partition the variability_ in the response further.  Essentially, we know of another reason that the responses vary in the way that they do: observations from the same subject should be related.  We want to tease this apart, and that is done by adding additional terms into the model for the data generating process.

Consider the following model:

$$
\begin{aligned}
  (\text{Taste Rating})_i &= \mu_1 (\text{East Side})_i + \mu_2 (\text{South Side})_i + \mu_3 (\text{Name Brand})_i \\
    &\qquad + \beta_2 (\text{Subject 2})_i + \beta_3 (\text{Subject 3})_i + \beta_4 (\text{Subject 4})_i \\
    &\qquad + \beta_5 (\text{Subject 5})_i + \beta_6 (\text{Subject 6})_i + \beta_7 (\text{Subject 7})_i \\
    &\qquad + \beta_8 (\text{Subject 8})_i + \beta_9 (\text{Subject 9})_i + \epsilon_i
\end{aligned}
$$

where

$$(\text{Subject j})_i = \begin{cases}
  1 & \text{i-th observation taken from Subject j} \\
  0 & \text{i-th observation not taken from subject j}
  \end{cases}$$
  
is an indicator of whether the observation comes from a particular subject.  It may at first appear as if we forgot the indicator for Subject 1; it is not needed.  Just as with any model, it is often easiest to see what is happening by thinking about the form of the model under specific cases.  How do we describe observations (remember there is more than one) for Subject 2?  Subject 2 adheres to the following model:

$$(\text{Taste Rating})_i = \mu_1 (\text{East Side})_i + \mu_2 (\text{South Side})_i + \mu_3 (\text{Name Brand})_i + \beta_2 + \epsilon_i$$

We could do something similar for each of Subjects 3 through 9.  What about Subject 1?  Well, Subject 1 would not be Subject 2 or 3, etc.  So, the indicator variable $$(\text{Subject 2})_i$$ for observations corresponding to Subject 1 would be 0; as would the indicators $$(\text{Subject 3})_i$$, $$(\text{Subject 4})_i$$, etc.  That is, Subject 1 adheres to the following model:

$$(\text{Taste Rating})_i = \mu_1 (\text{East Side})_i + \mu_2 (\text{South Side})_i + \mu_3 (\text{Name Brand})_i + \epsilon_i$$

This affects how we interpret our parameters.  In our model $\mu_1$ is no longer the average rating given to East Side Yogurt; it is the average rating given to East Side Yogurt _by Subject 1_.  It is the same concept as the "reference group" (see Definition \@ref(def:defn-reference-group)) discussed in the regression unit.

```{block2, type="rmdtip"}
If there are $b$ blocks, we need only include $b-1$ indicator variables and corresponding parameters to capture all the blocks.  The remaining block is the "refernece group" and is captured by the parameters comparing the factor levels under study.
```

This may seem like it affects our questions of interest.  After all, the hypothesis

$$H_0: \mu_1 = \mu_2 = \mu_3$$

says that the "average taste rating for Subject 1 is the same for all vendors" instead of the "average taste rating across all subjects is the same for all vendors."  The latter is the hypothesis we want to test, but we have the parameters specified in terms of the first subject only.  Notice, however, that subjects differ only by a constant (the $\beta$'s).  Therefore, if the mean response for one subject is the same for all vendors, then it must be that the mean response across vendors is the same for all subjects, addressing our question of interest.

```{block2, type="rmdtip"}
To formalize this mathematically, consider a case in which there are two groups and two blocks.  Then, we have a model of the form

$$(\text{Response})_i = \mu_1 (\text{Group 1})_i + \mu_2 (\text{Group 2})_i + \beta_2 (\text{Block 2})_i + \epsilon_i$$
  
Suppose we want the mean response across all blocks.  Then, that would be

$$(1/2)(\text{Mean from Block 1}) + (1/2)(\text{Mean from Block 2})$$
  
But, we can write this in terms of the parameters above.

$$
\begin{aligned}
  (\text{Mean for Group 1}) &= (1/2)\left(\mu_1\right) + (1/2)\left(\mu_1 + \beta_2\right) \\
  (\text{Mean for Group 2}) &= (1/2)\left(\mu_2\right) + (1/2)\left(\mu_2 + \beta_2\right) 
\end{aligned}
$$
  
If the mean response for each group is the same, then 

$$\mu_1 + (1/2)\beta_2 = \mu_2 + (1/2)\beta_2$$
  
But, this is only true if $\mu_1 = \mu_2$.  So, testing that the mean response is the same in the first block is equivalent to testing if the mean response is the same for all blocks.
```

This model essentially says there are three reasons that the taste ratings differ from one observation to another:

  - Ratings applied to different vendors differ.
  - Ratings from different subjects differ.
  - Unexplained differences due to inherent variability.
  
In general, this type of model, often described as a "Repeated Measures ANOVA" model, partitions the variability in the response into three general categories: differences between groups, differences between blocks, differences within blocks.

```{block2, type="rmdtip"}
In general, given a quantitative response variable, our model for the data generating process comparing this variable across $k$ levels of a factor in the presence of $b$ blocks is

$$(\text{Response})_i = \sum_{j=1}^{k} \mu_j (\text{Group})_{j, i} + \sum_{m=2}^{b} \beta_m (\text{Block})_{m, i} + \epsilon_i$$
  
where

$$
\begin{aligned}
  (\text{Group})_{j,i} &= \begin{cases}
    1 & \text{if i-th observation belongs to group j} \\
    0 & \text{if i-th observation does not belong to group j}
    \end{cases} \\
  (\text{Block})_{m,i} &= \begin{cases}
    1 & \text{if i-th observation belongs to block m} \\
    0 & \text{if i-th observation does not belong to block m}
    \end{cases}
\end{aligned}
$$

are indicators capturing the factor grouping and blocks.  This is often referred to as a "Repeated Measures ANOVA" model.
```
  
In the past, the stochastic portion of the model $\epsilon_i$ captured the subject-to-subject variability.  It no longer has the same role in this case.  It now captures the variability in repeated measures on the same subject.  That is, it captures the fact that if we repeatedly taste the same yogurt, we might rate it differently each time because of our mood or some other external factor that we have not captured.  The subject-to-subject variability is captured by the $\beta$ parameters in the model.

```{block2, type="rmdkeyidea"}
In a model without repeated measures (blocks), the error term captures the subject-to-subject variability.  In a model with repeated measures, the error term captures the variability between observations on the same subject.
```

There is something else that is unique about the repeated measures ANOVA model.  We do not really care about all the parameters in the model.  Our question of interest is based on the parameters $\mu_1, \mu_2, \mu_3$.  We would never be interested in testing something of the form

$$H_0: \beta_2 = \beta_3 \qquad \text{vs.} \qquad H_1: \beta_2 \neq \beta_3$$

as this would be comparing Subject 2 to Subject 3.  Such a comparison would not be useful because it does not help researchers to know that two different subjects have different preferences for yogurt taste.  Said another way, we did not put the parameters $\beta_2, \dotsc, \beta_9$ into the model because they helped us address a particular question.  Instead, we put them in the model because they captured the observed relationship in the responses.  This is the difference between __fixed effects__ and __random effects__.

```{definition, label=defn-fixed-effect, name="Fixed Effect"}
A predictor in the model for which we are interested in specific values observed and the relationship it has with the response.  It is generally part of our question of interest.
```

```{definition, label=defn-random-effect, name="Random Effect"}
A predictor in the model used to capture the correlation induced due to an inherit characteristic that varies across the population.  We are generally not interested in specific values observed but see these as a random sample from some underlying population of possibilities.  
```

While the theory underlying the analysis of such models makes use of more technical definitions of these terms, these are sufficient for our purposes in the text and understanding the role of these two types of terms.  Notice that if we were to repeat the study, we would use the same three vendors, since they are a fundamental part of the question.  However, we would not need to use the same subjects in the sample; we would be satisfied with any random sample from the population.  So, the values "East Side Yogurt," "South Side Yogurt," and "Name Brand" (at least, the three vendors these represent) are of specific interest.  However, we do not care about "Subject 2" and "Subject 3."  These can be any two individuals from the population.  Therefore, for the [Frozen Yogurt Case Study](#CaseYogurt), the vendor is the fixed effect, while subject is the random effect.  

```{block2, type="rmdtip"}
We are rarely interested in comparing one block to another.  The effect for the block is placed in the model in order to account for the correlation between responses; the block effect is then generally a random effect.
```

Just as before, we need to distinguish between the model for the data generating process and the model for the sampling distribution of the parameter estimates or the null distribution for a standarized statistic.  While we need the latter for inference, the model for the data generating process is a necessary stepping stone along the way.

The parameters in this model are of course unknown.  They can be estimated using the method of least squares.  However, these estimates, as with any estimate, have variability associated with them.  In order to make inference, we must determine a suitable model for the sampling distribution (or null distribution if appropriate).  This is done by placing conditions on the stochastic portion of the model.



## Conditions on the Error Distribution
In our model for the data-generating process we incorporated a component $\epsilon$ to capture the noise within each block.  Since the error is a random variable (stochastic element), we know it has a distribution.  We typically assume a certain structure to this distribution.  The more assumptions we are willing to make, the easier the analysis, but the less likely our model is to be applicable to the actual data-generating process we have observed.  The conditions we make dictate how we conduct inference (the computation of a p-value or confidence interval).

The first condition we consider is that the noise attributed to one observed response for an individual is independent of the noise attributed to the observed response for any other individual.  This may seem counter-intuitive; this entire unit exists because we felt there was a correlation among the responses.  However, this condition is stating that once we account for the correlation induced by the blocks by incorporating the random effect into the model, the remaining noise is now independent.  We essentially partitioned out the correlated component, and what remains is now just independent noise.

The second condition that is typically placed on the distribution of the errors is that the errors are identically distributed.  Again, we introduced this condition in Chapters \@ref(MeanModels) and \@ref(Regconditions).  In particular, if the errors are not identically distributed, it is typically because the variability of the error differs for one value of the predictors compared to another.  Practically, this reveals itself as our response being more precise for some collection of the predictors.  As a result of focusing on the variability of the response for each predictor, this condition is often referred to as _homoskedasticity_ instead of the errors being identically distributed.

The third condition we might consider imposing is that the errors in the response follow a Normal distribution, as discussed in Chapter \@ref(Regconditions).  If this condition holds, it implies that within block, the distribution of the response for a particular group is itself is Normally distributed.  While this was a bit easier to visualize in the ANOVA model, it becomes difficult to visualize here.  That is because we are talking about the taste ratings for the Name Brand yogurt made by Subject 1 following a Normal distribution; however, we only observed a single response from Subject 1 related to the Name Brand.  This is similar to when we placed conditions on the general regression model.

As in regression modeling, we are not required to impose all three conditions in order to obtain a model for the sampling distribution.  Historically, however, all three conditions are imposed.


## Conditions on the Random Effects
The random effects we placed in the model partitioned the original error term further; the random effects capture the correlation while the remaining portion captures the noise in the block.  This means that in addition to conditions on the error term, we must also place conditions on the random effects, since they represent a random sample from a population.  That is, the random effects also have a distribution which must be constrained.  

The easiest way to think about the random effects is the "bump" attributed to that block.  Again, think about a person who just is not a fan of frozen yogurt.  Then, regardless of which vendor their yogurt comes from, their taste rating will tend to "bump" down compared to others.  The first condition we impose is that this "bump" is the same for all observations from this individual.  That is, their taste for frozen yogurt does not shift during the study.  More generally, we are saying that the random effect associated with a particular block is always the same.

The second condition we consider is that the bump for one subject has nothing to do with the bump for any other subject.  Practically, my taste for frozen yogurt is unaffected by anyone else's taste for frozen yogurt.  This can be accomplished in the study by ensuring that each person is alone when they do their taste ratings so that they are not influenced by others' reactions.  More generally, the random effect for one block is independent of the random effect for any other block.

It is generally assumed the random effects follow a Normal distribution and that they are also independent of the error.



## Classical Repeated Measures ANOVA Model
In the preceeding sections, we discussed conditions we could place on the stocastic portions of the data generating process.  Placing all conditions on the error term and random effects is what we refer to as the "Classical Repeated Measures ANOVA Model."

```{definition, label=defn-classical-repeated-measures, name="Classical Repeated Measures ANOVA Model"}
For a quantitative response and categorical variable with $k$ groups observed each in $b$ blocks, the classical Repeated Measures ANOVA model assumes the following data-generating process:

$$(\text{Response})_i = \sum_{j=1}^{k} \mu_j (\text{Group})_{j, i} + \sum_{m=2}^{b} \beta_m (\text{Block})_{m, i} + \epsilon_i$$
  
where

$$
\begin{aligned}
  (\text{Group})_{j,i} &= \begin{cases}
    1 & \text{if i-th observation belongs to group j} \\
    0 & \text{if i-th observation does not belong to group j}
    \end{cases} \\
  (\text{Block})_{m,i} &= \begin{cases}
    1 & \text{if i-th observation belongs to block m} \\
    0 & \text{if i-th observation does not belong to block m}
    \end{cases}
\end{aligned}
$$

are indicators capturing the factor grouping and blocks, and

  1. The errors are independent of one another.
  2. The errors are identically distributed.
  3. The errors follow a Normal Distribution.
  4. The random effects are identical for all observations within a block.
  5. The random effects are independent of one another.
  6. The random effects are independent of the errors.
  7. The random effects follow a Normal Distribution.
  
It is possible to relax these assumptions; however, this is what is commonly implemented in practice.
```



## Recap
We have covered a lot of ground in this chapter, and it is worth taking a moment to summarize the big ideas.  In order compare the mean response in each group, needed to account for the correlation induced by the blocks when developing the model for the data generating process.  Such a model further partitions the error term in the model into a random effect which captures the variability across the blocks and the remaining noise captures the variability within a block.

Certain conditions are placed on the distributions of these stochastic portions.  Depending on the set of conditions we impose will determine whether we develop an analytical model or an empirical model for the parameters in the model.  We still need to discuss how we compare the mean response of the groups based on this model. 
